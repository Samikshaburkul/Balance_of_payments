---
title: "Financial Modeling"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=F}
# The package "ezids" (EZ Intro to Data Science) includes some helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
# You will need to install it (once) from GitHub.
# library(devtools)
# devtools::install_github("physicsland/ezids")
# Then load the package in your R session.
library(ezids)
```


```{r setup, include=FALSE}
# Some of common RMD options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, message = F)
# Can globally set option for number display format.
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```

```{r}
# 1. Do not provide answers/comments inside code blocks (like here) -- those are notes between coders/self and will be ignored for grading. 
# 2. Make sure your knitr options are set to include all results/code to be graded in the final document.
# 3. All charts/graphs/tables should have appropriate titles/labels/captions. 
# 4. Compose your answers using inline R code instead of using the code-block output as much as you can. 
# 5. Your grade is also determined by the style. Even if you answer everything correctly, but the .html does not look appealing, you will not get full credit. Pay attention to the details that we mentioned in class/homework and in previous sample .Rmd files. For example, how to use #, ##, ###, ..., bold face, italics, inline codes, tables, ..., {results = "asis"}, use of colors in plots/ggplots, and so forth.
```

```{r}
df <- read.csv('BalanceOfPayment.csv', header = TRUE)
```

```{r}

# Apply the filter
filtered_data <- subset(df, grepl("Financial Account", Indicator, ignore.case = TRUE))

# Print unique values in the Indicator column after filtering

portfolio_investement <- subset(df, grepl("Portfolio Investment", Indicator, ignore.case = TRUE))
other_investment <- subset(df, grepl("Other Investment", Indicator, ignore.case = TRUE))
Net_investment <- subset(df, grepl("Balance from Financial Account", Indicator, ignore.case = TRUE))
portfolio_investement <- subset(portfolio_investement, !grepl("Supplementary Items", Indicator, ignore.case = TRUE))

print(unique(portfolio_investement$Indicator))
print(unique(other_investment$Indicator))
print(unique(Net_investment$Indicator))

```
```{r}
portfolio_investement[, c('X2015', 'X2016', 'X2017','X2018','X2019','X2020')] <- lapply(portfolio_investement[, c('X2015', 'X2016', 'X2017','X2018','X2019','X2020')], function(x) (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
```


```{r}
portfolio_investement$Cumulative_Indicator1 <- rowSums(portfolio_investement[, c("X2015", "X2016", "X2017", "X2018", "X2019", "X2020")])
portfolio_investement$Average_Indicator1 <- rowMeans(portfolio_investement[, c("X2015", "X2016", "X2017", "X2018", "X2019", "X2020")], na.rm = TRUE)

```


```{r}
portfolio_investement$Indicator<- as.factor(portfolio_investement$Indicator)
```


```{r}
# Load necessary libraries
library(tidyverse)

# Example: Predicting 'Capital Account, Total, Net, US Dollars' for 'Afghanistan' in 2021
# Preprocess the data (assuming the data is clean and relevant features are selected)
data_filtered <- portfolio_investement %>%
  filter(Country == "Afghanistan, Islamic Rep. of", 
         Indicator == "Financial Account, Portfolio Investment, Net Acquisition of Financial Assets, US Dollars")

# Convert data to a suitable format for regression
reg_data <- data_filtered %>%
  select(X2015, X2016, X2017, X2018, X2019, X2020) %>%
  gather(key = "Year", value = "Value") %>%
  mutate(Year = as.numeric(substring(Year, 2)))

# Splitting the data (assuming a simple chronological split here)
train_data <- filter(reg_data, Year < 2019)
test_data <- filter(reg_data, Year >= 2019)

```


```{r}
# Linear regression model
model <- lm(Value ~ Year, data = train_data)

# Evaluate the model
summary(model)

# Predictions
predictions <- predict(model, newdata = test_data)
```


```{r}
# Example for quadratic regression
polModel <- lm(Value ~ poly(Year, 2), data = train_data)
summary(polModel)
```

```{r}
library(forecast)
arima_model <- auto.arima(train_data$Value)
summary(arima_model)

```
```{r}
# Example for random forest
install.packages('randomForest')
library(randomForest)
rf_model <- randomForest(Value ~ Year, data = train_data)
summary(rf_model)
```

```{r}
# Make predictions on the test set
predictions <- predict(rf_model, newdata = test_data)

# Create a confusion matrix
conf_matrix <- table(predictions, test_data$response_variable)

# Display the confusion matrix
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")

# Calculate precision
precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
cat("Precision:", precision, "\n")

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])
cat("Recall:", recall, "\n")

# Calculate F1 score
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1 Score:", f1_score, "\n")

```









